# è¯­éŸ³å”¤é†’æœåŠ¡å™¨éƒ¨ç½²æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨ GPU äº‘æœåŠ¡å™¨ä¸Šéƒ¨ç½²å…¨åŠŸèƒ½è¯­éŸ³å¤„ç†æœåŠ¡ï¼ŒåŒ…å«ï¼š
- è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰- è¯­éŸ³è½¬æ–‡å­—
- å£°çº¹è¯†åˆ« - è¯†åˆ«/éªŒè¯è¯´è¯äºº
- è¯­éŸ³å¢å¼º - é™å™ªã€å»æ··å“
- æƒ…æ„Ÿè¯†åˆ« - è¯†åˆ«è¯­éŸ³æƒ…ç»ª
- è¯­è¨€è¯†åˆ« - è¯†åˆ«è¯´çš„ä»€ä¹ˆè¯­è¨€
- VAD - æ£€æµ‹æ˜¯å¦æœ‰äººè¯´è¯
- TTS - æ–‡å­—è½¬è¯­éŸ³

---

## æœåŠ¡å™¨ä¿¡æ¯

```
IP: 117.50.176.26
ç«¯å£: 23
ç”¨æˆ·: root
å¯†ç : Wsq7Se4vLmi96HT

è¿æ¥å‘½ä»¤: ssh -p 23 root@117.50.176.26
```

---

## ç¬¬ä¸€æ­¥ï¼šè¿æ¥æœåŠ¡å™¨

### æ–¹å¼ä¸€ï¼šå‘½ä»¤è¡Œ
```bash
ssh -p 23 root@117.50.176.26
# è¾“å…¥å¯†ç : Wsq7Se4vLmi96HT
```

### æ–¹å¼äºŒï¼šVS Code Remote-SSH
1. å®‰è£… Remote-SSH æ’ä»¶
2. é…ç½® `~/.ssh/config`:
```
Host gpu-server
    HostName 117.50.176.26
    User root
    Port 23
```
3. è¿æ¥ gpu-server

---

## ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ç¯å¢ƒ

```bash
# æ£€æŸ¥ GPU
nvidia-smi

# æ£€æŸ¥ Python
python --version

# æ£€æŸ¥ CUDA
nvcc --version
```

---

## ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºé¡¹ç›®ç›®å½•

```bash
mkdir -p /root/voice-wake-server
cd /root/voice-wake-server
```

---

## ç¬¬å››æ­¥ï¼šå®‰è£…æ‰€æœ‰ä¾èµ–

```bash
# å‡çº§ pip
pip install --upgrade pip

# å®‰è£… PyTorchï¼ˆCUDA 12.x ç‰ˆæœ¬ï¼‰
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121

# ============ æ ¸å¿ƒæ¨¡å— ============

# Whisper - è¯­éŸ³è¯†åˆ«ï¼ˆä¸­æ–‡æ•ˆæœæœ€å¥½ï¼‰
pip install openai-whisper

# Faster-Whisper - åŠ é€Ÿç‰ˆè¯­éŸ³è¯†åˆ«ï¼ˆæ¨èï¼Œé€Ÿåº¦å¿« 2-4 å€ï¼‰
pip install faster-whisper

# SpeechBrain - å£°çº¹è¯†åˆ«ã€è¯­éŸ³å¢å¼ºã€æƒ…æ„Ÿè¯†åˆ«ç­‰
pip install speechbrain

# ============ è¾…åŠ©æ¨¡å— ============

# Silero VAD - è¯­éŸ³æ´»åŠ¨æ£€æµ‹
pip install silero-vad

# TTS - æ–‡å­—è½¬è¯­éŸ³
pip install TTS

# è¯­è¨€è¯†åˆ«
pip install langid

# ============ Web æœåŠ¡ ============

# FastAPI Web æ¡†æ¶
pip install fastapi uvicorn websockets python-multipart

# å…¶ä»–å·¥å…·
pip install numpy scipy librosa soundfile
```

### ä¸€é”®å®‰è£…å‘½ä»¤

```bash
pip install --upgrade pip && \
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121 && \
pip install openai-whisper faster-whisper speechbrain silero-vad TTS langid && \
pip install fastapi uvicorn websockets python-multipart numpy scipy librosa soundfile
```

---

## ç¬¬äº”æ­¥ï¼šåˆ›å»ºæœåŠ¡ç«¯ä»£ç 

```bash
cat > /root/voice-wake-server/server.py << 'ENDOFFILE'
"""
å…¨åŠŸèƒ½è¯­éŸ³å¤„ç†æœåŠ¡å™¨
åŠŸèƒ½ï¼š
1. è¯­éŸ³è¯†åˆ«ï¼ˆWhisper/Faster-Whisperï¼‰
2. å£°çº¹è¯†åˆ«ï¼ˆSpeechBrainï¼‰
3. è¯­éŸ³å¢å¼º/é™å™ªï¼ˆSpeechBrainï¼‰
4. æƒ…æ„Ÿè¯†åˆ«ï¼ˆSpeechBrainï¼‰
5. è¯­è¨€è¯†åˆ«ï¼ˆlangidï¼‰
6. VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆSileroï¼‰
7. TTS æ–‡å­—è½¬è¯­éŸ³
"""

import torch
import numpy as np
import io
import json
import asyncio
import logging
import tempfile
import soundfile as sf
from pathlib import Path
from typing import Optional, Dict, Any
from fastapi import FastAPI, WebSocket, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="å…¨åŠŸèƒ½è¯­éŸ³å¤„ç†æœåŠ¡å™¨", version="1.0.0")

# è·¨åŸŸé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============ å…¨å±€æ¨¡å‹ ============
models = {
    "whisper": None,           # è¯­éŸ³è¯†åˆ«
    "speaker": None,           # å£°çº¹è¯†åˆ«
    "enhancer": None,          # è¯­éŸ³å¢å¼º
    "emotion": None,           # æƒ…æ„Ÿè¯†åˆ«
    "vad": None,               # VAD
    "tts": None,               # TTS
}

# ç”¨æˆ·é…ç½®å­˜å‚¨
user_configs: Dict[str, Dict[str, Any]] = {}

def get_user_config(user_id: str) -> Dict[str, Any]:
    if user_id not in user_configs:
        user_configs[user_id] = {
            "wake_word": "ä½ å¥½æ˜Ÿå¹´",
            "voice_embedding": None,
            "voice_enabled": False,
        }
    return user_configs[user_id]

# ============ æ¨¡å‹åŠ è½½ ============

@app.on_event("startup")
async def load_models():
    """å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    global models
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½ Faster-Whisperï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰
    logger.info("æ­£åœ¨åŠ è½½ Faster-Whisper æ¨¡å‹...")
    try:
        from faster_whisper import WhisperModel
        models["whisper"] = WhisperModel("base", device=device, compute_type="float16")
        logger.info("âœ… Faster-Whisper åŠ è½½å®Œæˆ")
    except Exception as e:
        logger.warning(f"Faster-Whisper åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åŸç‰ˆ Whisper: {e}")
        import whisper
        models["whisper"] = whisper.load_model("base", device=device)
        logger.info("âœ… Whisper åŠ è½½å®Œæˆ")
    
    # 2. åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹...")
    try:
        from speechbrain.inference.speaker import SpeakerRecognition
        models["speaker"] = SpeakerRecognition.from_hparams(
            source="speechbrain/spkrec-ecapa-voxceleb",
            savedir="/root/voice-wake-server/models/speaker",
            run_opts={"device": device}
        )
        logger.info("âœ… å£°çº¹è¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        logger.error(f"å£°çº¹è¯†åˆ«æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # 3. åŠ è½½è¯­éŸ³å¢å¼ºæ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½è¯­éŸ³å¢å¼ºæ¨¡å‹...")
    try:
        from speechbrain.inference.enhancement import SpectralMaskEnhancement
        models["enhancer"] = SpectralMaskEnhancement.from_hparams(
            source="speechbrain/metricgan-plus-voicebank",
            savedir="/root/voice-wake-server/models/enhancer",
            run_opts={"device": device}
        )
        logger.info("âœ… è¯­éŸ³å¢å¼ºæ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        logger.warning(f"è¯­éŸ³å¢å¼ºæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # 4. åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹...")
    try:
        from speechbrain.inference.classifiers import EncoderClassifier
        models["emotion"] = EncoderClassifier.from_hparams(
            source="speechbrain/emotion-recognition-wav2vec2-IEMOCAP",
            savedir="/root/voice-wake-server/models/emotion",
            run_opts={"device": device}
        )
        logger.info("âœ… æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        logger.warning(f"æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # 5. åŠ è½½ VAD æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½ VAD æ¨¡å‹...")
    try:
        vad_model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )
        models["vad"] = {"model": vad_model, "utils": utils}
        logger.info("âœ… VAD æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        logger.warning(f"VAD æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # 6. åŠ è½½ TTS æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½ TTS æ¨¡å‹...")
    try:
        from TTS.api import TTS
        models["tts"] = TTS(model_name="tts_models/zh-CN/baker/tacotron2-DDC-GST", progress_bar=False)
        if device == "cuda":
            models["tts"].to(device)
        logger.info("âœ… TTS æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        logger.warning(f"TTS æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼ŒæœåŠ¡å°±ç»ªï¼")

# ============ å·¥å…·å‡½æ•° ============

def audio_bytes_to_numpy(audio_bytes: bytes, sample_rate: int = 16000) -> np.ndarray:
    """å°†éŸ³é¢‘å­—èŠ‚è½¬æ¢ä¸º numpy æ•°ç»„"""
    audio_np = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
    return audio_np

def save_temp_audio(audio_np: np.ndarray, sample_rate: int = 16000) -> str:
    """ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶"""
    temp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    sf.write(temp_file.name, audio_np, sample_rate)
    return temp_file.name

# ============ æ ¸å¿ƒåŠŸèƒ½ ============

def transcribe_audio(audio_np: np.ndarray) -> dict:
    """è¯­éŸ³è¯†åˆ«"""
    if models["whisper"] is None:
        return {"text": "", "error": "æ¨¡å‹æœªåŠ è½½"}
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ faster-whisper
        if hasattr(models["whisper"], 'transcribe'):
            # faster-whisper
            segments, info = models["whisper"].transcribe(
                audio_np,
                language="zh",
                beam_size=5
            )
            text = "".join([seg.text for seg in segments])
            return {"text": text.strip(), "language": info.language}
        else:
            # åŸç‰ˆ whisper
            result = models["whisper"].transcribe(audio_np, language="zh", fp16=True)
            return {"text": result["text"].strip(), "language": "zh"}
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
        return {"text": "", "error": str(e)}

def get_speaker_embedding(audio_np: np.ndarray) -> Optional[torch.Tensor]:
    """è·å–å£°çº¹ç‰¹å¾"""
    if models["speaker"] is None:
        return None
    try:
        audio_tensor = torch.from_numpy(audio_np).unsqueeze(0)
        embedding = models["speaker"].encode_batch(audio_tensor)
        return embedding
    except Exception as e:
        logger.error(f"å£°çº¹æå–é”™è¯¯: {e}")
        return None

def verify_speaker(emb1: torch.Tensor, emb2: torch.Tensor, threshold: float = 0.5) -> dict:
    """å£°çº¹éªŒè¯"""
    score = torch.nn.functional.cosine_similarity(emb1, emb2).item()
    return {"is_same": score > threshold, "score": score}

def enhance_audio(audio_np: np.ndarray) -> np.ndarray:
    """è¯­éŸ³å¢å¼º/é™å™ª"""
    if models["enhancer"] is None:
        return audio_np
    try:
        audio_tensor = torch.from_numpy(audio_np).unsqueeze(0)
        enhanced = models["enhancer"].enhance_batch(audio_tensor)
        return enhanced.squeeze().numpy()
    except Exception as e:
        logger.error(f"è¯­éŸ³å¢å¼ºé”™è¯¯: {e}")
        return audio_np

def recognize_emotion(audio_np: np.ndarray) -> dict:
    """æƒ…æ„Ÿè¯†åˆ«"""
    if models["emotion"] is None:
        return {"emotion": "unknown", "score": 0}
    try:
        temp_file = save_temp_audio(audio_np)
        out_prob, score, index, label = models["emotion"].classify_file(temp_file)
        Path(temp_file).unlink()  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        return {"emotion": label[0], "score": score.item()}
    except Exception as e:
        logger.error(f"æƒ…æ„Ÿè¯†åˆ«é”™è¯¯: {e}")
        return {"emotion": "unknown", "score": 0}

def detect_language(text: str) -> dict:
    """è¯­è¨€è¯†åˆ«"""
    try:
        import langid
        lang, confidence = langid.classify(text)
        return {"language": lang, "confidence": confidence}
    except Exception as e:
        return {"language": "unknown", "confidence": 0}

def detect_vad(audio_np: np.ndarray, sample_rate: int = 16000) -> dict:
    """VAD æ£€æµ‹"""
    if models["vad"] is None:
        return {"has_speech": True, "segments": []}
    try:
        vad_model = models["vad"]["model"]
        get_speech_timestamps = models["vad"]["utils"][0]
        
        audio_tensor = torch.from_numpy(audio_np)
        speech_timestamps = get_speech_timestamps(audio_tensor, vad_model, sampling_rate=sample_rate)
        
        has_speech = len(speech_timestamps) > 0
        segments = [{"start": ts["start"] / sample_rate, "end": ts["end"] / sample_rate} for ts in speech_timestamps]
        
        return {"has_speech": has_speech, "segments": segments}
    except Exception as e:
        logger.error(f"VAD æ£€æµ‹é”™è¯¯: {e}")
        return {"has_speech": True, "segments": []}

def text_to_speech(text: str, output_path: str = None) -> Optional[str]:
    """æ–‡å­—è½¬è¯­éŸ³"""
    if models["tts"] is None:
        return None
    try:
        if output_path is None:
            output_path = tempfile.NamedTemporaryFile(suffix=".wav", delete=False).name
        models["tts"].tts_to_file(text=text, file_path=output_path)
        return output_path
    except Exception as e:
        logger.error(f"TTS é”™è¯¯: {e}")
        return None

# ============ HTTP API ============

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    loaded_models = [k for k, v in models.items() if v is not None]
    return {
        "status": "ok",
        "message": "è¯­éŸ³å¤„ç†æœåŠ¡å™¨è¿è¡Œä¸­",
        "loaded_models": loaded_models
    }

@app.post("/set_wake_word")
async def set_wake_word(user_id: str = Form(...), wake_word: str = Form(...)):
    """è®¾ç½®å”¤é†’è¯"""
    config = get_user_config(user_id)
    config["wake_word"] = wake_word
    logger.info(f"ç”¨æˆ· {user_id} è®¾ç½®å”¤é†’è¯: {wake_word}")
    return {"status": "ok", "wake_word": wake_word}

@app.post("/register_voice")
async def register_voice(user_id: str = Form(...), audio: UploadFile = File(...)):
    """æ³¨å†Œå£°çº¹"""
    config = get_user_config(user_id)
    audio_bytes = await audio.read()
    audio_np = audio_bytes_to_numpy(audio_bytes)
    
    embedding = get_speaker_embedding(audio_np)
    if embedding is not None:
        config["voice_embedding"] = embedding
        config["voice_enabled"] = True
        logger.info(f"ç”¨æˆ· {user_id} æ³¨å†Œå£°çº¹æˆåŠŸ")
        return {"status": "ok", "message": "å£°çº¹æ³¨å†ŒæˆåŠŸ"}
    else:
        raise HTTPException(status_code=500, detail="å£°çº¹æå–å¤±è´¥")

@app.post("/recognize")
async def recognize(
    user_id: str = Form(...),
    audio: UploadFile = File(...),
    enhance: bool = Form(False),
    check_emotion: bool = Form(False)
):
    """å®Œæ•´è¯­éŸ³è¯†åˆ«ï¼ˆHTTP æ–¹å¼ï¼‰"""
    config = get_user_config(user_id)
    audio_bytes = await audio.read()
    audio_np = audio_bytes_to_numpy(audio_bytes)
    
    result = {}
    
    # VAD æ£€æµ‹
    vad_result = detect_vad(audio_np)
    result["vad"] = vad_result
    
    if not vad_result["has_speech"]:
        result["text"] = ""
        result["wake_detected"] = False
        return result
    
    # è¯­éŸ³å¢å¼ºï¼ˆå¯é€‰ï¼‰
    if enhance:
        audio_np = enhance_audio(audio_np)
        result["enhanced"] = True
    
    # è¯­éŸ³è¯†åˆ«
    asr_result = transcribe_audio(audio_np)
    result["text"] = asr_result["text"]
    result["asr_language"] = asr_result.get("language", "zh")
    
    # æ£€æŸ¥å”¤é†’è¯
    result["wake_detected"] = config["wake_word"] in result["text"]
    result["wake_word"] = config["wake_word"]
    
    # å£°çº¹éªŒè¯
    result["speaker_verified"] = False
    result["speaker_score"] = 0.0
    if result["wake_detected"] and config["voice_enabled"] and config["voice_embedding"] is not None:
        current_embedding = get_speaker_embedding(audio_np)
        if current_embedding is not None:
            verify_result = verify_speaker(config["voice_embedding"], current_embedding)
            result["speaker_verified"] = verify_result["is_same"]
            result["speaker_score"] = verify_result["score"]
    
    # æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¯é€‰ï¼‰
    if check_emotion:
        emotion_result = recognize_emotion(audio_np)
        result["emotion"] = emotion_result
    
    # è¯­è¨€è¯†åˆ«
    if result["text"]:
        lang_result = detect_language(result["text"])
        result["text_language"] = lang_result
    
    return result

@app.post("/enhance")
async def enhance(audio: UploadFile = File(...)):
    """è¯­éŸ³å¢å¼º/é™å™ª"""
    audio_bytes = await audio.read()
    audio_np = audio_bytes_to_numpy(audio_bytes)
    
    enhanced_np = enhance_audio(audio_np)
    
    # è¿”å›å¢å¼ºåçš„éŸ³é¢‘
    output_path = save_temp_audio(enhanced_np)
    return FileResponse(output_path, media_type="audio/wav", filename="enhanced.wav")

@app.post("/tts")
async def tts(text: str = Form(...)):
    """æ–‡å­—è½¬è¯­éŸ³"""
    output_path = text_to_speech(text)
    if output_path:
        return FileResponse(output_path, media_type="audio/wav", filename="tts_output.wav")
    else:
        raise HTTPException(status_code=500, detail="TTS ç”Ÿæˆå¤±è´¥")

@app.post("/emotion")
async def emotion(audio: UploadFile = File(...)):
    """æƒ…æ„Ÿè¯†åˆ«"""
    audio_bytes = await audio.read()
    audio_np = audio_bytes_to_numpy(audio_bytes)
    result = recognize_emotion(audio_np)
    return result

@app.post("/vad")
async def vad(audio: UploadFile = File(...)):
    """VAD æ£€æµ‹"""
    audio_bytes = await audio.read()
    audio_np = audio_bytes_to_numpy(audio_bytes)
    result = detect_vad(audio_np)
    return result

# ============ WebSocket API ============

@app.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    """WebSocket å®æ—¶è¯­éŸ³è¯†åˆ«"""
    await websocket.accept()
    config = get_user_config(user_id)
    logger.info(f"ç”¨æˆ· {user_id} WebSocket è¿æ¥")
    
    try:
        while True:
            audio_bytes = await websocket.receive_bytes()
            audio_np = audio_bytes_to_numpy(audio_bytes)
            
            # VAD æ£€æµ‹
            vad_result = detect_vad(audio_np)
            if not vad_result["has_speech"]:
                await websocket.send_json({"wake_detected": False, "has_speech": False})
                continue
            
            # è¯­éŸ³è¯†åˆ«
            asr_result = transcribe_audio(audio_np)
            text = asr_result["text"]
            
            # æ£€æŸ¥å”¤é†’è¯
            wake_detected = config["wake_word"] in text
            
            # å£°çº¹éªŒè¯
            speaker_verified = False
            speaker_score = 0.0
            if wake_detected and config["voice_enabled"] and config["voice_embedding"] is not None:
                current_embedding = get_speaker_embedding(audio_np)
                if current_embedding is not None:
                    verify_result = verify_speaker(config["voice_embedding"], current_embedding)
                    speaker_verified = verify_result["is_same"]
                    speaker_score = verify_result["score"]
            
            result = {
                "text": text,
                "wake_detected": wake_detected,
                "speaker_verified": speaker_verified,
                "speaker_score": speaker_score,
                "has_speech": True
            }
            await websocket.send_json(result)
            
            if wake_detected:
                logger.info(f"ğŸ¤ ç”¨æˆ· {user_id} å”¤é†’: {text}, å£°çº¹: {speaker_verified}")
                
    except Exception as e:
        logger.error(f"WebSocket é”™è¯¯: {e}")
    finally:
        logger.info(f"ç”¨æˆ· {user_id} WebSocket æ–­å¼€")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
ENDOFFILE
```

---

## ç¬¬å…­æ­¥ï¼šå¯åŠ¨æœåŠ¡

```bash
cd /root/voice-wake-server

# å‰å°è¿è¡Œï¼ˆæµ‹è¯•ï¼‰
python server.py

# åå°è¿è¡Œ
nohup python server.py > server.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f server.log
```

---

## ç¬¬ä¸ƒæ­¥ï¼šæµ‹è¯• API

### å¥åº·æ£€æŸ¥
```bash
curl http://localhost:8000/
```

### è®¾ç½®å”¤é†’è¯
```bash
curl -X POST http://localhost:8000/set_wake_word \
  -F "user_id=user001" \
  -F "wake_word=ä½ å¥½æ˜Ÿå¹´"
```

### æ³¨å†Œå£°çº¹
```bash
curl -X POST http://localhost:8000/register_voice \
  -F "user_id=user001" \
  -F "audio=@my_voice.wav"
```

### è¯­éŸ³è¯†åˆ«ï¼ˆå®Œæ•´åŠŸèƒ½ï¼‰
```bash
curl -X POST http://localhost:8000/recognize \
  -F "user_id=user001" \
  -F "audio=@test.wav" \
  -F "enhance=true" \
  -F "check_emotion=true"
```

### è¯­éŸ³å¢å¼º
```bash
curl -X POST http://localhost:8000/enhance \
  -F "audio=@noisy.wav" \
  --output enhanced.wav
```

### æ–‡å­—è½¬è¯­éŸ³
```bash
curl -X POST http://localhost:8000/tts \
  -F "text=ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åŠ©æ‰‹" \
  --output output.wav
```

### æƒ…æ„Ÿè¯†åˆ«
```bash
curl -X POST http://localhost:8000/emotion \
  -F "audio=@test.wav"
```

### VAD æ£€æµ‹
```bash
curl -X POST http://localhost:8000/vad \
  -F "audio=@test.wav"
```

---

## API æ–‡æ¡£

### HTTP æ¥å£

| æ¥å£ | æ–¹æ³• | å‚æ•° | è¯´æ˜ |
|------|------|------|------|
| `/` | GET | - | å¥åº·æ£€æŸ¥ |
| `/set_wake_word` | POST | user_id, wake_word | è®¾ç½®å”¤é†’è¯ |
| `/register_voice` | POST | user_id, audio | æ³¨å†Œå£°çº¹ |
| `/recognize` | POST | user_id, audio, enhance?, check_emotion? | å®Œæ•´è¯­éŸ³è¯†åˆ« |
| `/enhance` | POST | audio | è¯­éŸ³å¢å¼º/é™å™ª |
| `/tts` | POST | text | æ–‡å­—è½¬è¯­éŸ³ |
| `/emotion` | POST | audio | æƒ…æ„Ÿè¯†åˆ« |
| `/vad` | POST | audio | VAD æ£€æµ‹ |

### WebSocket æ¥å£

| æ¥å£ | è¯´æ˜ |
|------|------|
| `/ws/{user_id}` | å®æ—¶è¯­éŸ³è¯†åˆ« |

---

## åŠŸèƒ½è¯´æ˜

| åŠŸèƒ½ | æ¨¡å‹ | è¯´æ˜ |
|------|------|------|
| è¯­éŸ³è¯†åˆ« | Faster-Whisper base | è¯­éŸ³è½¬æ–‡å­—ï¼Œæ”¯æŒä¸­æ–‡ |
| å£°çº¹è¯†åˆ« | ECAPA-TDNN | è¯´è¯äººè¯†åˆ«/éªŒè¯ |
| è¯­éŸ³å¢å¼º | MetricGAN+ | é™å™ªã€å»æ··å“ |
| æƒ…æ„Ÿè¯†åˆ« | Wav2Vec2 | è¯†åˆ«å–œæ€’å“€ä¹ |
| VAD | Silero VAD | æ£€æµ‹æ˜¯å¦æœ‰äººè¯´è¯ |
| TTS | Tacotron2 | ä¸­æ–‡æ–‡å­—è½¬è¯­éŸ³ |
| è¯­è¨€è¯†åˆ« | langid | è¯†åˆ«æ–‡å­—è¯­è¨€ |

---

## å¸¸è§é—®é¢˜

### Q: CUDA out of memory
A: å‡å°‘åŒæ—¶åŠ è½½çš„æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q: æ¨¡å‹ä¸‹è½½æ…¢
A: ä½¿ç”¨å›½å†…é•œåƒæˆ–æå‰ä¸‹è½½æ¨¡å‹æ–‡ä»¶

### Q: TTS ä¸­æ–‡æ•ˆæœä¸å¥½
A: å¯ä»¥æ¢ç”¨ edge-ttsï¼ˆå¾®è½¯ TTSï¼‰

---

## ä¸‹ä¸€æ­¥

1. é…ç½®å…¬ç½‘è®¿é—®ï¼ˆç«¯å£æ˜ å°„ï¼‰
2. ä¿®æ”¹ ESP32 ä»£ç ä¸Šä¼ éŸ³é¢‘
3. æ·»åŠ ç”¨æˆ·è®¤è¯å’Œæ•°æ®åº“
